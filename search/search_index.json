{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ezp","text":"<p><code>ezp</code> is a lightweight C++ wrapper for selected distributed solvers for linear systems.</p>"},{"location":"#features","title":"Features","text":"<ol> <li>easy to use interface</li> <li>drop-in header-only library</li> <li>standalone solver binaries that can be invoked by various callers</li> <li>random tested implementation</li> </ol> <p>The following solvers are implemented.</p> availability type of matrix operation solver package \u2705 general (partial pivoting) simple PxGESV ScaLAPACK \u2705 general (partial pivoting) expert PxGESVX ScaLAPACK \u2705 symmetric/Hermitian positive definite simple PxPOSV ScaLAPACK \u2705 symmetric/Hermitian positive definite expert PxPOSVX ScaLAPACK \u2705 general band (partial pivoting) simple PxGBSV ScaLAPACK \u2705 general band (no pivoting) simple PxDBSV ScaLAPACK \u2705 symmetric/Hermitian positive definite band simple PxPBSV ScaLAPACK \u2705 sparse PARDISO MKL \u2705 sparse MUMPS MUMPS"},{"location":"#dependency","title":"Dependency","text":"<p>The <code>ezp</code> library requires C++ 20 compatible compiler. The following drivers are needed.</p> <ol> <li>an implementation of <code>LAPACK</code> and <code>BLAS</code>, such as <code>OpenBLAS</code>, <code>MKL</code>, etc.</li> <li>an implementation of <code>ScaLAPACK</code></li> <li>an implementation of <code>MPI</code>, such as <code>OpenMPI</code>, <code>MPICH</code>, etc.</li> </ol>"},{"location":"#example","title":"Example","text":"<p>It is assumed that the root node (rank 0) prepares the left hand side \\(\\(A\\)\\) and right hand side \\(\\(B\\)\\). The solvers distribute the matrices to available processes and solve the system, return the solution back to the master node.</p> <p>The solvers are designed in such a way that all <code>BLACS</code> and <code>ScaLAPACK</code> details are hidden. One shall prepare the matrices (on the root node) and call the solver. The following is a typical example. It highly resembles the sequential version of how one would typically solve a linear system.</p> <p>The following is a working example.</p> C++<pre><code>#include &lt;ezp/pgesv.hpp&gt;\n#include &lt;iomanip&gt;\n#include &lt;iostream&gt;\n\nusing namespace ezp;\n\nint main() {\n    // get the current blacs environment\n    const auto rank = get_env&lt;int&gt;().rank();\n\n    constexpr auto N = 6, NRHS = 2;\n\n    // storage for the matrices A and B\n    std::vector&lt;double&gt; A, B;\n\n    if(0 == rank) {\n        // the matrices are only initialized on the root process\n        A.resize(N * N, 0.);\n        B.resize(N * NRHS, 1.);\n\n        // helper functor to convert 2D indices to 1D indices\n        // it's likely the matrices are provided by some other subsystem\n        const auto IDX = par_dgesv&lt;int&gt;::indexer{N};\n\n        for(auto I = 0; I &lt; N; ++I) A[IDX(I, I)] = static_cast&lt;double&gt;(I);\n    }\n\n    // create a parallel solver\n    // it takes the number of rows and columns of the process grid as arguments\n    // or let the library automatically determine as follows\n    // need to wrap the data in full_mat objects\n    // it requires the number of rows and columns of the matrix, and a pointer to the data\n    // on non-root processes, the data pointer is nullptr as the vector is empty\n    // par_dgesv&lt;int&gt;().solve(full_mat{N, N, A.data()}, full_mat{N, NRHS, B.data()});\n    par_dgesv&lt;int&gt;().solve({N, N, A.data()}, {N, NRHS, B.data()});\n\n    if(0 == rank) {\n        std::cout &lt;&lt; std::setprecision(10) &lt;&lt; \"Solution:\\n\";\n        for(auto i = 0; i &lt; B.size(); ++i) std::cout &lt;&lt; B[i] &lt;&lt; '\\n';\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"Guide/Custom/","title":"Custom Matrix Class","text":"<p><code>ezp</code> supports customised matrix objects.</p> <p>Apart from using the provided wrappers such as <code>full_mat</code>, <code>band_mat</code> and <code>band_symm_mat</code>, one can also define or use customised matrix classes to call the solver.</p> <p>All custom matrix classes shall have the public member <code>.n_rows</code> and <code>.n_cols</code>. The band matrix further requires the public member <code>.kl</code> and <code>.ku</code> which are the numbers of sub-diagonals (lower bandwidth) and super-diagonals (upper bandwidth). The band symmetric matrix requires the additional public member <code>.klu</code> which is the bandwidth.</p> <p>All custom classes must define at least one of the following public methods, that return a pointer to the first element. It is assumed the memory layout is contiguous, thus, one can use <code>std::vector&lt;T&gt;</code>, <code>std::array&lt;T&gt;</code>, <code>std::unique_ptr&lt;T[]&gt;</code> as the storage.</p> <ol> <li><code>.mem()</code> -&gt; <code>T*</code></li> <li><code>.memptr()</code> -&gt; <code>T*</code></li> <li><code>.data()</code> -&gt; <code>T*</code></li> <li>contiguous iterator pair <code>.begin()</code> and <code>.end()</code>, and <code>&amp;(*begin())</code> -&gt; <code>T*</code></li> </ol> <p>The possibility is unlimited. The simplest is to subclass <code>std::vector&lt;T&gt;</code> with additional public members. In the example of <code>posv</code> solver, a custom matrix class is used.</p> example.pposv.cpp:27:54<pre><code>#include &lt;ezp/pposv.hpp&gt;\n#include &lt;iomanip&gt;\n#include &lt;iostream&gt;\n\nusing namespace ezp;\n\nclass mat {\npublic:\n    int_t n_rows, n_cols;\n\nprivate:\n    std::vector&lt;double&gt; storage;\n\npublic:\n    mat(const int_t rows, const int cols)\n        : n_rows(rows)\n        , n_cols(cols) {}\n\n    auto init() { storage.resize(n_rows * n_cols); }\n\n    auto&amp; operator()(const int_t i, const int_t j) { return storage[i + j * n_cols]; }\n\n    auto&amp; operator[](const int_t i) { return storage[i]; }\n\n    auto begin() { return storage.begin(); }\n\n    auto end() { return storage.end(); }\n};\n</code></pre> <p>It shall be noted that, all processes need to have the same dimensional inputs. Thus, <code>.n_rows</code> and <code>.n_cols</code> shall be valid. However, the actual storage is only initialised on the root process.</p> example.pposv.cpp:56:91<pre><code>int main() {\n    // get the current blacs environment\n    const auto&amp; env = get_env&lt;int_t&gt;();\n\n    constexpr auto N = 6, NRHS = 2;\n\n    // storage for the matrices A and B using the custom class\n    // acceptable objects shall have members `.n_rows` and `.n_cols`\n    // and have any of the following methods `.mem()`, `.memptr()`, `.data()`\n    // or contiguous iterators\n    // all of above methods shall return a pointer to the first element\n    mat A(N, N), B(N, NRHS);\n\n    if(0 == env.rank()) {\n        // the matrices are only initialized on the root process\n        A.init();\n        B.init();\n\n        for(auto I = 0; I &lt; N; ++I) {\n            B[I] = A(I, I) = I + 1;\n            B[I + N] = I + 4;\n        }\n    }\n\n    // create a parallel solver\n    // and send custom matrix objects to the solver\n    const auto info = par_dposv&lt;int_t&gt;().solve(A, B);\n\n    if(0 == env.rank() &amp;&amp; 0 == info) {\n        std::cout &lt;&lt; std::setprecision(10) &lt;&lt; \"Info: \" &lt;&lt; info &lt;&lt; '\\n';\n        std::cout &lt;&lt; \"Solution:\\n\";\n        for(const auto i : B) std::cout &lt;&lt; i &lt;&lt; '\\n';\n    }\n\n    return info;\n}\n</code></pre>"},{"location":"Guide/MPI/","title":"MPI Environment Management","text":""},{"location":"Guide/MPI/#initialisation","title":"Initialisation","text":"<p>It is not required to explicitly initialise either the <code>MPI</code> environment via <code>MPI_Init</code> or the <code>BLACS</code> environment via <code>blacs_gridinit</code>. All those operations are automatically done via <code>RAII</code>.</p> <p>Use the following to get the rank and size of the current <code>BLACS</code> environment if needed.</p> C++<pre><code>ezp::get_env&lt;int&gt;().rank();\nezp::get_env&lt;int&gt;().size();\n</code></pre>"},{"location":"Guide/MPI/#finalisation","title":"Finalisation","text":"<p>Finalisation is a bit more complex. When several MPI based libraries are used together, each may have its own initialisation and finalisation processes. Then who is in charge of finalising the <code>MPI</code> resources matters. And the order of calling different finalisation functions matters. Unfortunately, with <code>RAII</code>, there is no reliable way to explicitly assign an order such that, for example, <code>blacs_exit</code> is guaranteed to be called before <code>MPI_Finalize</code>.</p> <p>However, it is possible to tell <code>ezp</code> to skip finalising <code>MPI</code>. To do so, call the following function anywhere in your application.</p> C++<pre><code>ezp::blacs_env&lt;int&gt;::do_not_manage_mpi();\n</code></pre> <p>Once this is called, <code>ezp</code> will memorise the setting and call <code>blacs_exit(1)</code> instead of <code>blacs_exit(0)</code> on destruction, the latter not only releases <code>blacs</code> resources but also further calls <code>MPI_Finalize</code>.</p> <p>By such, you shall manually call <code>MPI_Finalize</code>, or probably let other libraries to finalise the <code>MPI</code> environment for you.</p>"},{"location":"Guide/Standalone/","title":"Standalone Solver","text":"<p>MPI provides a mechanism to dynamically spawn worker processes via <code>MPI_Comm_spawn</code>. By utilising this feature, it is possible to separate the main application from the solver. The architecture can be illustrated as follows.</p> <p></p> <p>The <code>esp</code> solvers are also available as standalone executables that follow the above design. Such a form is particularly beneficial if the following holds.</p> <ol> <li>The main application is not suitable for distributed-memory model due to, for example, latency constraints.</li> <li>The main application adopts a different parallelism pattern.</li> <li>Sometimes, one may want to have a clear boundary between the main application and the solver(s).</li> </ol> <p>However, such a clear boundary may be a shortcoming as well. For example, the root process needs to store the whole matrices in memory, which is subject to physical limitations. And since the spawned processes are not persistent, solving the same system with different right-hand sides may incur unnecessary data communication.</p> <p>If the matrices need to be read from IO in a distributed manner, one may also need a refined control of how the data is prepared, by probably directly using <code>BLACS</code>/<code>ScaLAPACK</code>/<code>MPI</code> functions.</p>"},{"location":"Guide/Standalone/#backbone","title":"Backbone","text":"<p>Here, we use the <code>pgesv</code> solver as the example to illustrate some critical steps used in implementing the above architecture.</p> <p>We are not using the raw <code>MPI</code> functions, instead, the <code>mpl</code> library is used. It is necessary to get the default communicator and the parent inter communicator.</p> solver.full.hpp:24:25<pre><code>inline const auto&amp; comm_world{mpl::environment::comm_world()};\ninline const auto&amp; parent = mpl::inter_communicator::parent();\n</code></pre> <p>Since <code>mpl</code> handles <code>MPI</code> environment initialisation and finalisation, it is necessary to tell <code>ezp</code> to ignore finalising the <code>MPI</code> environment. The function <code>ezp::blacs_env::do_not_manage_mpi()</code> must be called if <code>MPI</code> is managed by external tools.</p> solver.pgesv.cpp:54<pre><code>    ezp::blacs_env&lt;int&gt;::do_not_manage_mpi();\n</code></pre> <p>The parameters of the linear system will be broadcast over by the main application first.</p> solver.pgesv.cpp:61:65<pre><code>    const auto all = mpl::communicator(parent, mpl::communicator::order_high);\n\n    int config[3]{};\n\n    all.bcast(0, config);\n</code></pre> <p>Knowing the problem sizes <code>N</code> and <code>NRHS</code>, the root process can initialise the containers and receive the contents of two matrices.</p> solver.full.hpp:28:40<pre><code>    std::vector&lt;DT&gt; A, B;\n\n    if(0 == comm_world.rank()) {\n        A.resize(N * N);\n        B.resize(N * NRHS);\n\n        mpl::irequest_pool requests;\n\n        requests.push(parent.irecv(A, 0, mpl::tag_t{0}));\n        requests.push(parent.irecv(B, 0, mpl::tag_t{1}));\n\n        requests.waitall();\n    }\n</code></pre> <p>Solving the system follows the conventional approach, that is, create a solver object and call the solve method with data wrapped. The solution is sent back to the caller.</p> solver.full.hpp:42:47<pre><code>    const auto error = solver_t().solve({N, N, A.data()}, {N, NRHS, B.data()});\n\n    if(0 == comm_world.rank()) {\n        parent.send(error, 0);\n        if(0 == error) parent.send(B, 0);\n    }\n</code></pre>"},{"location":"Guide/Standalone/#the-caller","title":"The Caller","text":"<p>From the above code snippets, one may observe that the caller needs to</p> <ol> <li>broadcast <code>config</code>,</li> <li>send <code>A</code> and <code>B</code>,</li> <li>receive the error code and the solution <code>X</code> if no error occured.</li> </ol> <p>To this end, one can declare the corresponding containers using <code>std::vector</code>.</p> runner.cpp:38:39<pre><code>    std::vector&lt;double&gt; A, B(N * NRHS, 1.);\n    std::vector&lt;int&gt; config;\n</code></pre> <p>The following creates a diagonal matrix for illustration.</p> runner.cpp:45:50<pre><code>        solver = \"solver.pgesv\";\n\n        config = {N, NRHS, 1};\n\n        A.resize(N * N, 0.);\n        for(auto I = 0; I &lt; N; ++I) A[I * N + I] = I + 1.;\n</code></pre> <p>Now since all the data is ready to be communicated, the actual communication is very concise and straightforward.</p> runner.cpp:111:126<pre><code>    const auto&amp; comm_world{mpl::environment::comm_world()};\n    const auto worker = comm_world.spawn(0, argc &lt; 3 ? 1 : std::abs(std::stoi(argv[2])), {solver});\n    const auto all = mpl::communicator(worker, mpl::communicator::order_low);\n\n    all.bcast(0, config.data(), mpl::contiguous_layout&lt;int&gt;(config.size()));\n\n    mpl::irequest_pool requests;\n\n    requests.push(worker.isend(A, 0, mpl::tag_t{0}));\n    requests.push(worker.isend(B, 0, mpl::tag_t{1}));\n\n    requests.waitall();\n\n    int error = -1;\n    worker.recv(error, 0);\n    if(0 == error) worker.recv(B, 0);\n</code></pre>"},{"location":"Guide/Standalone/#python-caller","title":"Python Caller","text":"<p>Since there is a clear boundary between the main application and the solver, it is possible to use other MPI compatible to send/receive the problem.</p> <p>For example, one can use <code>mpi4py</code> to call the solvers.</p> <p>It has to be pointed out that, the column-major memory layout shall be kept.</p> runner.py<pre><code>#\n# This script runs a parallel solver using MPI to solve a system of linear equations.\n#\n# Functions:\n#     run(nprocs: int, N: int, NRHS: int):\n#         Spawns a parallel solver process, sends matrix data, and receives the solution.\n#\n# Usage:\n#     runner.py &lt;nprocs&gt; &lt;N&gt; &lt;NRHS&gt;\n#\n# Arguments:\n#     nprocs (int): Number of processes to spawn.\n#     N (int): Size of the NxN matrix.\n#     NRHS (int): Number of right-hand sides.\n#\n# Example:\n#     python runner.py 4 10 2\n#\n\nimport numpy\nimport sys\nfrom array import array\nfrom mpi4py import MPI\n\n\ndef run(nprocs: int, N: int, NRHS: int):\n    comm = MPI.COMM_WORLD\n\n    # spawn the solver\n    worker = comm.Spawn(\"./solver.pgesv\", maxprocs=nprocs)\n    all = worker.Merge()\n\n    # broadcast the problem configuration\n    all.Bcast(array(\"i\", [N, NRHS, 1]), root=0)\n\n    # send the matrices\n    # ! numpy arrays are row-major by default\n    # ! column-major order is required by (Sca)LAPACK\n    # ! thus need to specify the order as \"F\"\n    A = numpy.zeros((N, N), dtype=numpy.float64, order=\"F\")\n    for i in range(N):\n        A[i, i] = i + 1\n    print(\"A:\\n\", A)\n    worker.Send(A, dest=0, tag=0)\n\n    B = numpy.ones((N, NRHS), dtype=numpy.float64, order=\"F\")\n    for i in range(NRHS):\n        B[:, i] = i + 1\n    print(\"B:\\n\", B)\n    worker.Send(B, dest=0, tag=1)\n\n    # receive the error code and the solution if no error\n    error = array(\"i\", [-1])\n    worker.Recv(error)\n    if 0 == error[0]:\n        X = numpy.ones((N, NRHS), dtype=numpy.float64, order=\"F\")\n        worker.Recv(X)\n        print(\"Solution:\\n\", X)\n\n    worker.Disconnect()\n    all.Disconnect()\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) &lt; 4:\n        print(\"Usage: runner.py &lt;nprocs&gt; &lt;N&gt; &lt;NRHS&gt;\")\n        sys.exit(1)\n\n    nprocs = int(sys.argv[1])\n    N = int(sys.argv[2])\n    NRHS = int(sys.argv[3])\n\n    run(nprocs, N, NRHS)\n</code></pre> <p>Calling the above script with</p> Bash<pre><code>./runner.py 2 6 2\n</code></pre> <p>yields the following is the solution, which is correct.</p> Text Only<pre><code>A:\n [[1. 0. 0. 0. 0. 0.]\n [0. 2. 0. 0. 0. 0.]\n [0. 0. 3. 0. 0. 0.]\n [0. 0. 0. 4. 0. 0.]\n [0. 0. 0. 0. 5. 0.]\n [0. 0. 0. 0. 0. 6.]]\nB:\n [[1. 2.]\n [1. 2.]\n [1. 2.]\n [1. 2.]\n [1. 2.]\n [1. 2.]]\nSolution:\n [[1.         2.        ]\n [0.5        1.        ]\n [0.33333333 0.66666667]\n [0.25       0.5       ]\n [0.2        0.4       ]\n [0.16666667 0.33333333]]\n</code></pre>"},{"location":"Guide/Standalone/#full-reference-implementation","title":"Full Reference Implementation","text":"<p>The following is a full reference implementation of a standalone solver and the corresponding caller logic.</p> runner.cpp runner.cpp<pre><code>#include &lt;iostream&gt;\n#include &lt;mpl/mpl.hpp&gt;\n\nint main(int argc, char** argv) {\n    if(argc &lt; 2) {\n        std::cout &lt;&lt; \"Usage: runner ge|gex|po|pox|gb|db|pb [n]\\n\";\n        std::cout &lt;&lt; \"Example: runner ge 3\\n\";\n        return 0;\n    }\n    constexpr auto N = 6, NRHS = 1;\n\n    std::vector&lt;double&gt; A, B(N * NRHS, 1.);\n    std::vector&lt;int&gt; config;\n\n    const auto type = std::string(argv[1]);\n    std::string solver;\n\n    if(\"ge\" == type) {\n        solver = \"solver.pgesv\";\n\n        config = {N, NRHS, 1};\n\n        A.resize(N * N, 0.);\n        for(auto I = 0; I &lt; N; ++I) A[I * N + I] = I + 1.;\n    }\n    else if(\"gex\" == type) {\n        solver = \"solver.pgesvx\";\n\n        config = {N, NRHS, 1};\n\n        A.resize(N * N, 0.);\n        for(auto I = 0; I &lt; N; ++I) A[I * N + I] = I + 1.;\n    }\n    else if(\"po\" == type) {\n        solver = \"solver.pposv\";\n\n        config = {N, NRHS, 1};\n\n        A.resize(N * N, 0.);\n        for(auto I = 0; I &lt; N; ++I) A[I * N + I] = I + 1.;\n    }\n    else if(\"pox\" == type) {\n        solver = \"solver.pposvx\";\n\n        config = {N, NRHS, 1};\n\n        A.resize(N * N, 0.);\n        for(auto I = 0; I &lt; N; ++I) A[I * N + I] = I + 1.;\n    }\n    else if(\"gb\" == type) {\n        solver = \"solver.pgbsv\";\n\n        constexpr auto KL = 1, KU = 1;\n\n        config = {N, KL, KU, NRHS, 1};\n\n        A.resize(N * (KL + KU + 1), 0.);\n        for(auto I = 0; I &lt; N; ++I) A[KU + I * (KL + KU + 1)] = I + 1.;\n    }\n    else if(\"db\" == type) {\n        solver = \"solver.pdbsv\";\n\n        constexpr auto KL = 1, KU = 1;\n\n        config = {N, KL, KU, NRHS, 1};\n\n        A.resize(N * (KL + KU + 1), 0.);\n        for(auto I = 0; I &lt; N; ++I) A[KU + I * (KL + KU + 1)] = I + 1.;\n    }\n    else if(\"pb\" == type) {\n        solver = \"solver.ppbsv\";\n\n        constexpr auto KLU = 1;\n\n        config = {N, KLU, NRHS, 1};\n\n        A.resize(N * (KLU + 1), 0.);\n        for(auto I = 0; I &lt; N; ++I) A[I + I * KLU] = I + 1.;\n    }\n    else {\n        std::cout &lt;&lt; \"Usage: runner ge|gex|po|pox|gb|db|pb [n]\\n\";\n        return 0;\n    }\n\n    const auto&amp; comm_world{mpl::environment::comm_world()};\n    const auto worker = comm_world.spawn(0, argc &lt; 3 ? 1 : std::abs(std::stoi(argv[2])), {solver});\n    const auto all = mpl::communicator(worker, mpl::communicator::order_low);\n\n    all.bcast(0, config.data(), mpl::contiguous_layout&lt;int&gt;(config.size()));\n\n    mpl::irequest_pool requests;\n\n    requests.push(worker.isend(A, 0, mpl::tag_t{0}));\n    requests.push(worker.isend(B, 0, mpl::tag_t{1}));\n\n    requests.waitall();\n\n    int error = -1;\n    worker.recv(error, 0);\n    if(0 == error) worker.recv(B, 0);\n\n    for(auto I = 0; I &lt; N; ++I) printf(\"x[%d] = %+.6f\\n\", I, B[I]);\n\n    return 0;\n}\n</code></pre> solver.pgesv.cpp solver.pgesv.cpp<pre><code>#include \"solver.full.hpp\"\n\nint main(int, char**) {\n    ezp::blacs_env&lt;int&gt;::do_not_manage_mpi();\n\n    if(!parent.is_valid()) {\n        printf(\"This program must be invoked by the host application.\\n\");\n        return 0;\n    }\n\n    const auto all = mpl::communicator(parent, mpl::communicator::order_high);\n\n    int config[3]{};\n\n    all.bcast(0, config);\n\n    const auto N = config[0];\n    const auto NRHS = config[1];\n    const auto FLOAT = config[2];\n\n    if(FLOAT &gt;= 10) return run&lt;complex16, int_t, ezp::pgesv&lt;complex16, int_t&gt;&gt;(N, NRHS);\n    if(FLOAT &gt;= 0) return run&lt;double, int_t, ezp::pgesv&lt;double, int_t&gt;&gt;(N, NRHS);\n    if(FLOAT &gt; -10) return run&lt;float, int_t, ezp::pgesv&lt;float, int_t&gt;&gt;(N, NRHS);\n\n    return run&lt;complex8, int_t, ezp::pgesv&lt;complex8, int_t&gt;&gt;(N, NRHS);\n}\n</code></pre>"},{"location":"Intro/Compilation/","title":"Compilation","text":"<p>As <code>ezp</code> is wrapper library of <code>ScaLAPACK</code>, the following dependencies are necessary to compile any executables that use <code>ezp</code>.</p> <ol> <li>an implementation of <code>LAPACK</code> and <code>BLAS</code>, such as <code>OpenBLAS</code>, <code>Intel\u00ae oneAPI Math Kernel Library (oneMKL)</code>, <code>AMD Optimizing CPU Libraries (AOCL)</code>, etc.</li> <li>an implementation of <code>ScaLAPACK</code>, such as the reference implementation, Intel's implementation, NVIDIA's implementation, etc.</li> <li>an implementation of <code>MPI</code>, such as <code>OpenMPI</code>, <code>MPICH</code>, <code>Intel\u00ae MPI</code> etc.</li> </ol> <p>Before compiling the executables, one must ensure those libraries are available.</p>"},{"location":"Intro/Compilation/#oneapi","title":"oneAPI","text":"<p>The easiest approach is to use the <code>Intel\u00ae oneAPI</code> toolkit. It provides the <code>Intel\u00ae MPI Library</code> and the <code>Intel\u00ae oneAPI Math Kernel Library (oneMKL)</code> which contains a complete <code>ScaLAPACK</code> and <code>LAPACK</code>/<code>BLAS</code> implementation.</p> <p>The following is a sample workflow that runs on <code>Fedora</code>.</p> Bash<pre><code>echo -e \"[oneAPI]\\nname=Intel\u00ae oneAPI repository\\nbaseurl=https://yum.repos.intel.com/oneapi\\nenabled=1\\ngpgcheck=1\\nrepo_gpgcheck=1\\ngpgkey=https://yum.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB\" &gt; /etc/yum.repos.d/oneAPI.repo\n\nsudo dnf install -y intel-oneapi-mkl-devel intel-oneapi-mpi-devel cmake gcc g++ gfortran git ninja-build\n\ngit clone --recurse-submodules --depth 1 https://github.com/TLCFEM/ezp.git\n\nmkdir /ezp/build &amp;&amp; /ezp/build\n\n. /opt/intel/oneapi/setvars.sh &amp;&amp; cmake -DEZP_TEST=ON .. &amp;&amp; cmake --build . --config Release\n</code></pre>"},{"location":"Intro/Compilation/#system-libraries","title":"System Libraries","text":"<p>Using system libraries is possible but most bundled libraries are broken/outdated on various distros. It's likely the bundled system libraries do not work for various compatibility issues. To circumvent, one then needs to manually compile all those dependencies before using them. This is cumbersome.</p> <p><code>openSUSE</code> has a functioning toolset. Simply install the library and you are ready to go.</p> Bash<pre><code>sudo zypper in -y libscalapack2_2_2_0-gnu-mpich-hpc-devel-static git cmake\ngit clone --recurse-submodules --depth 1 https://github.com/TLCFEM/ezp.git\nmkdir /ezp/build &amp;&amp; /ezp/build\ncmake -DEZP_TEST=ON -DEZP_USE_SYSTEM_LIBS=ON -DMPI_HOME=/usr/lib/hpc/gnu14/mpi/mpich/4.3.0/ -DCMAKE_PREFIX_PATH=\"/usr/lib/hpc/gnu14/mpich/scalapack/2.2.0/lib64/;/usr/lib/hpc/gnu14/openblas/0.3.29/lib64/\" .. &amp;&amp; cmake --build . --config Release\n</code></pre> <p><code>Fedora 41</code> has an environment that is close to usable. The following is an example that uses system packages.</p> Bash<pre><code>sudo dnf install -y cmake gcc g++ gfortran git ninja-build scalapack-mpich-devel flexiblas-devel\n# sadly the scalapack package is broken on fedora 41\n# need to provide this link\nsudo ln -s /usr/lib64/mpich/lib/libscalapack.so.2.2.0 /usr/lib64/libscalapack.so.2.2.0\n\ngit clone --recurse-submodules --depth 1 https://github.com/TLCFEM/ezp.git\nmkdir /ezp/build &amp;&amp; /ezp/build\ncmake -DEZP_TEST=ON -DMPI_HOME=/usr/lib64/mpich/ -DEZP_USE_SYSTEM_LIBS=ON .. &amp;&amp; cmake --build . --config Release\n</code></pre> <p>Other mainstream distros require more fixes. Still, it is not recommended due to the lack of flexibility. It may not be possible/feasible to switch to another implementation of any of those libraries.</p>"},{"location":"Intro/Compilation/#compilation-options","title":"Compilation Options","text":""},{"location":"Intro/Compilation/#64-bit-integer","title":"64-bit Integer","text":"<p>Use <code>-DEZP_USE_64BIT_INT=ON</code> flag in CMake, or define the macro <code>EZP_INT64</code>.</p>"},{"location":"Intro/Compilation/#name-mangling","title":"Name Mangling","text":"<p>Use <code>-DEZP_ADD_UNDERSCORE=ON</code> flag in CMake, or define the macro <code>EZP_UNDERSCORE</code>.</p>"},{"location":"Intro/Compilation/#standalone-solvers","title":"Standalone Solvers","text":"<p>Use <code>-DEZP_STANDALONE=ON</code> flag in CMake to compile standalone solver.</p>"},{"location":"Intro/Motivation/","title":"Motivation","text":"<p>As of this writing, it is necessary to directly call <code>ScaLAPACK</code> subroutines via the <code>C</code> interface if one wants to solve linear systems using <code>ScaLAPACK</code>. Since both <code>MPI</code> and <code>LAPACK</code>/<code>BLAS</code> have fabulous <code>C++</code> wrappers (see <code>mpl</code> and <code>Armadillo</code>), it would be nice if <code>ScaLAPACK</code> can be used in a similar OO manner.</p> <p>Such a wrapper is advantageous and beneficial in the context of <code>C++</code> applications due to the following reasons.</p> <ol> <li>It avoids direct interactions with the <code>C</code> API, which requires manual memory management.    The caller has to follow pretty much the same procedural style.</li> <li>A well-designed OO approach can significantly reduce the code size.</li> </ol> <p><code>ezp</code> is designed to provide an intuitive interface that resembles the non-MPI version of linear systems solvers. The majority of <code>ScaLAPACK</code>, <code>BLACS</code> and <code>MPI</code> communication details shall be well hidden from the caller so that solving a system can be as simple as <code>solver.solve(A, B)</code>. Of course, a complete, working example would be longer than a single line of code, due to the distributed-memory nature of MPI. However, as shown in the front page, a minimum example spans merely a few lines of code.</p> C++<pre><code>#include &lt;ezp/pgesv.hpp&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    constexpr auto N = 6, NRHS = 2;\n\n    std::vector&lt;double&gt; A, B;\n\n    if(0 == ezp::get_env&lt;int&gt;().rank()) {\n        A.resize(N * N, 0.);\n        B.resize(N * NRHS, 1.);\n\n        const auto IDX = ezp::par_dgesv&lt;int&gt;::indexer{N};\n        for(auto I = 0; I &lt; N; ++I) A[IDX(I, I)] = I;\n    }\n\n    ezp::par_dgesv&lt;int&gt;().solve({N, N, A.data()}, {N, NRHS, B.data()});\n\n    if(0 == ezp::get_env&lt;int&gt;().rank()) for(const auto i : B) std::cout &lt;&lt; i &lt;&lt; '\\n';\n\n    return 0;\n}\n</code></pre>"},{"location":"Intro/Motivation/#available-solvers","title":"Available Solvers","text":"<p><code>ezp</code> implements all precisions (DSZC) with both 32-bit and 64-bit indexing.</p>"},{"location":"Intro/Motivation/#dense-matrix","title":"Dense Matrix","text":"<p><code>ezp</code> provides all solvers listed in ScaLAPCK page for full and band matrices. Solvers for tridiagonal matrices are not implemented as they can be stored in band formats and solved by the corresponding solver.</p>"},{"location":"Intro/Motivation/#sparse-matrix","title":"Sparse Matrix","text":"<p><code>ezp</code> provides interface to <code>PARDISO</code> direct solver bundled in Intel\u00ae oneAPI Math Kernel Library (oneMKL).</p> <p><code>ezp</code> provides interface to <code>MUMPS</code> direct solver.</p> <p><code>ezp</code> provides interface to <code>Lis</code> iterative solver.</p>"},{"location":"Intro/Motivation/#why-not-superlu","title":"Why not SuperLU?","text":"<p>The codebase is messy and hard to maintain due to, for example, abuse of macros. And it is not significantly faster than other direct solvers. I do not like its code aesthetics.</p>"},{"location":"Intro/Motivation/#references","title":"References","text":"<ol> <li>Configuration of a linear solver for linearly implicit time integration and efficient data transfer in parallel thermo-hydraulic computations</li> <li>A Parallel Geometric Multifrontal Solver Using Hierarchically Semiseparable Structure</li> <li>Caveats of three direct linear solvers for finite element analyses</li> <li>Evaluating Accuracy and Efficiency of HPC Solvers for Sparse Linear Systems with Applications to PDEs</li> </ol>"}]}